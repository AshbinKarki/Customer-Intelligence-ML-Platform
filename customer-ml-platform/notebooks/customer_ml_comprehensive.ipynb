{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb00e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow Setup\n",
    "from src.utils.mlflow_tracker import MLflowTracker\n",
    "\n",
    "print(\"ðŸ”¬ MLOps Pipeline Setup\\n\")\n",
    "\n",
    "# Initialize MLflow\n",
    "tracker = MLflowTracker()\n",
    "tracker.start_experiment(\"customer-churn-v1\")\n",
    "\n",
    "# Log models\n",
    "print(\"ðŸ“ Logging models to MLflow...\")\n",
    "tracker.log_segmentation_model(\n",
    "    kmeans_result['model'],\n",
    "    model_name=\"kmeans_segmentation\",\n",
    "    metrics=kmeans_result['metrics'],\n",
    "    algorithm=\"kmeans\"\n",
    ")\n",
    "\n",
    "tracker.log_churn_model(\n",
    "    xgb_model,\n",
    "    model_name=\"xgboost_churn\",\n",
    "    metrics=xgb_metrics,\n",
    "    model_type=\"xgboost\"\n",
    ")\n",
    "\n",
    "tracker.log_churn_model(\n",
    "    lgb_model,\n",
    "    model_name=\"lightgbm_churn\",\n",
    "    metrics=lgb_metrics,\n",
    "    model_type=\"lightgbm\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Models logged to MLflow\")\n",
    "\n",
    "# Model Comparison Report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š FINAL MODEL COMPARISON REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'LightGBM', 'Random Forest', 'Logistic Regression'],\n",
    "    'Accuracy': [xgb_metrics['accuracy'], lgb_metrics['accuracy'], rf_metrics['accuracy'], lr_metrics['accuracy']],\n",
    "    'AUC-ROC': [xgb_metrics['roc_auc'], lgb_metrics['roc_auc'], rf_metrics['roc_auc'], lr_metrics['roc_auc']],\n",
    "    'F1-Score': [xgb_metrics['f1'], lgb_metrics['f1'], rf_metrics['f1'], lr_metrics['f1']],\n",
    "    'Precision': [xgb_metrics['precision'], lgb_metrics['precision'], rf_metrics['precision'], lr_metrics['precision']],\n",
    "    'Recall': [xgb_metrics['recall'], lgb_metrics['recall'], rf_metrics['recall'], lr_metrics['recall']]\n",
    "})\n",
    "\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# Deployment Checklist\n",
    "print(\"\\n\\nðŸ“‹ DEPLOYMENT CHECKLIST:\")\n",
    "print(\"âœ“ Models trained and evaluated\")\n",
    "print(\"âœ“ Feature engineering pipeline created\")\n",
    "print(\"âœ“ Segmentation models (K-Means, DBSCAN, Hierarchical)\")\n",
    "print(\"âœ“ Churn prediction models (XGBoost, LightGBM, RF)\")\n",
    "print(\"âœ“ Recommendation system (Collaborative Filtering)\")\n",
    "print(\"âœ“ NLP sentiment analysis model\")\n",
    "print(\"âœ“ Real-time prediction capability\")\n",
    "print(\"âœ“ Batch processing pipeline\")\n",
    "print(\"âœ“ MLflow experiment tracking\")\n",
    "print(\"âœ“ API endpoints defined (FastAPI)\")\n",
    "print(\"âœ“ Docker containerization setup\")\n",
    "print(\"âœ“ Kubernetes manifests created\")\n",
    "print(\"âœ“ Airflow DAGs for orchestration\")\n",
    "print(\"âœ“ Prometheus monitoring configuration\")\n",
    "print(\"\\nâœ… System ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ecf659",
   "metadata": {},
   "source": [
    "## 9. MLOps Pipeline & Model Deployment\n",
    "\n",
    "Setting up experiment tracking, model registry, and deployment considerations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17faaee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time Inference\n",
    "print(\"âš¡ REAL-TIME INFERENCE\\n\")\n",
    "\n",
    "# Sample customer\n",
    "sample_customer = customers_engineered.iloc[0]\n",
    "X_sample_engineered = fe.create_customer_features(pd.DataFrame([sample_customer]))\n",
    "X_sample, _ = fe.prepare_for_modeling(X_sample_engineered, fit_scaler=False)\n",
    "\n",
    "# Predictions\n",
    "xgb_pred = xgb_model.predict(X_sample)[0]\n",
    "xgb_proba = xgb_model.predict_proba(X_sample)[0, 1]\n",
    "\n",
    "print(f\"Customer ID: {sample_customer['customer_id']}\")\n",
    "print(f\"Total Purchases: {sample_customer['total_purchases']}\")\n",
    "print(f\"Customer Lifetime Value: ${sample_customer['customer_lifetime_value']:.2f}\")\n",
    "print(f\"\\nðŸŽ¯ Churn Prediction:\")\n",
    "print(f\"  Prediction: {'âš ï¸ CHURN RISK' if xgb_pred == 1 else 'âœ“ SAFE'}\")\n",
    "print(f\"  Probability: {xgb_proba:.1%}\")\n",
    "\n",
    "# Batch Inference\n",
    "from src.data.batch_processor import BatchProcessor\n",
    "\n",
    "print(\"\\n\\nðŸ“¦ BATCH INFERENCE\\n\")\n",
    "\n",
    "batch_processor = BatchProcessor()\n",
    "\n",
    "# Create test CSV\n",
    "test_batch = customers_engineered.head(100).copy()\n",
    "test_batch.to_csv('data/raw/batch_test.csv', index=False)\n",
    "\n",
    "# Run batch predictions\n",
    "print(\"Processing batch churn predictions...\")\n",
    "batch_results = batch_processor.batch_churn_prediction('data/raw/batch_test.csv')\n",
    "\n",
    "print(\"\\nBatch Results Summary:\")\n",
    "print(f\"Total Customers: {len(batch_results)}\")\n",
    "print(f\"High Risk (>70%): {(batch_results['churn_probability'] > 0.7).sum()}\")\n",
    "print(f\"Medium Risk (40-70%): {((batch_results['churn_probability'] > 0.4) & (batch_results['churn_probability'] <= 0.7)).sum()}\")\n",
    "print(f\"Low Risk (<40%): {(batch_results['churn_probability'] < 0.4).sum()}\")\n",
    "\n",
    "print(\"\\nSample Batch Results:\")\n",
    "print(batch_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea32b5b",
   "metadata": {},
   "source": [
    "## 8. Real-time & Batch Inference\n",
    "\n",
    "Demonstrating prediction serving and batch processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b711b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Confusion Matrices\n",
    "print(\"ðŸ” Confusion Matrices for Churn Models:\\n\")\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\"XGBoost\", \"LightGBM\", \"Random Forest\", \"Logistic Regression\"),\n",
    "    specs=[[{'type': 'heatmap'}, {'type': 'heatmap'}],\n",
    "           [{'type': 'heatmap'}, {'type': 'heatmap'}]]\n",
    ")\n",
    "\n",
    "models_list = [\n",
    "    ('xgboost', xgb_model),\n",
    "    ('lightgbm', lgb_model),\n",
    "    ('random_forest', rf_model),\n",
    "    ('logistic_regression', lr_model)\n",
    "]\n",
    "\n",
    "positions = [(1,1), (1,2), (2,1), (2,2)]\n",
    "\n",
    "for (model_name, model), (row, col) in zip(models_list, positions):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=cm, text=cm, texttemplate='%{text}', colorscale='Blues'),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=600, title_text=\"Confusion Matrices\", showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# Classification Reports\n",
    "print(\"ðŸ“Š Detailed Classification Report (XGBoost):\\n\")\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['No Churn', 'Churn']))\n",
    "\n",
    "# Cross-Validation\n",
    "print(\"\\nðŸ”„ Cross-Validation Scores (5-fold):\")\n",
    "cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "print(f\"XGBoost AUC: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d4eb8",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation & Comprehensive Reporting\n",
    "\n",
    "Detailed model evaluation with cross-validation and diagnostic plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.nlp import SentimentAnalyzer, TopicModeling, AspectBasedSentiment\n",
    "\n",
    "# Sentiment Analysis\n",
    "print(\"ðŸ’¬ Training Sentiment Analysis Model...\")\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "\n",
    "# Encode sentiment labels\n",
    "sentiment_le = LabelEncoder()\n",
    "y_sentiment = sentiment_le.fit_transform(reviews_df['sentiment'])\n",
    "\n",
    "# Train model\n",
    "sentiment_analyzer.train_sentiment_model(\n",
    "    reviews_df['review_text'],\n",
    "    y_sentiment,\n",
    "    model_type='logistic_regression'\n",
    ")\n",
    "print(\"âœ“ Sentiment model trained\")\n",
    "\n",
    "# Predict sentiment\n",
    "sample_reviews = [\n",
    "    \"Amazing product! Highly recommend to everyone.\",\n",
    "    \"Terrible quality, very disappointed.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "]\n",
    "\n",
    "print(\"\\nðŸŽ¯ Sample Sentiment Predictions:\")\n",
    "for review in sample_reviews:\n",
    "    preds, probs = sentiment_analyzer.predict_sentiment(pd.Series([review]))\n",
    "    sentiment = sentiment_le.inverse_transform(preds)[0]\n",
    "    confidence = probs[0].max()\n",
    "    print(f\"Review: '{review}'\")\n",
    "    print(f\"â†’ Sentiment: {sentiment.upper()} (Confidence: {confidence:.1%})\\n\")\n",
    "\n",
    "# Topic Modeling\n",
    "print(\"ðŸ“š Training LDA Topic Model...\")\n",
    "topic_model = TopicModeling(n_topics=5)\n",
    "topic_model.train_lda(reviews_df['review_text'])\n",
    "\n",
    "topics = topic_model.get_topics(n_words=5)\n",
    "print(\"\\nDiscovered Topics:\")\n",
    "for topic_id, words in topics.items():\n",
    "    print(f\"Topic {topic_id}: {', '.join(words)}\")\n",
    "\n",
    "# Aspect-Based Sentiment\n",
    "print(\"\\nðŸ” Aspect-Based Sentiment Analysis...\")\n",
    "aspect_analyzer = AspectBasedSentiment()\n",
    "aspect_sentiments = aspect_analyzer.analyze_aspect_sentiment(reviews_df)\n",
    "\n",
    "print(\"\\nAspect Sentiment Distribution:\")\n",
    "print(aspect_sentiments.groupby(['aspect', 'sentiment']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02a9c8",
   "metadata": {},
   "source": [
    "## 6. NLP Sentiment Analysis\n",
    "\n",
    "Customer review sentiment analysis and aspect-based opinion mining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.recommendation import RecommendationSystem\n",
    "\n",
    "# Initialize recommendation system\n",
    "recsys = RecommendationSystem(n_latent_factors=50)\n",
    "\n",
    "# Create user-item matrix\n",
    "print(\"ðŸ“¦ Building User-Item Matrix...\")\n",
    "matrix = recsys.create_user_item_matrix(purchases_df)\n",
    "print(f\"âœ“ Matrix shape: {matrix.shape}\")\n",
    "print(f\"âœ“ Sparsity: {1 - matrix.nnz / (matrix.shape[0] * matrix.shape[1]):.1%}\")\n",
    "\n",
    "# Train Collaborative Filtering (NMF)\n",
    "print(\"\\nðŸ¤ Training Collaborative Filtering (NMF)...\")\n",
    "cf_result = recsys.collaborative_filtering_nmf(matrix, n_components=50, alpha=0.0001)\n",
    "print(f\"âœ“ NMF RMSE: {cf_result['rmse']:.3f}\")\n",
    "\n",
    "# Get recommendations for sample customers\n",
    "print(\"\\nðŸŽ Sample Recommendations:\")\n",
    "sample_customers = [1, 10, 100, 500]\n",
    "for customer_id in sample_customers:\n",
    "    recs = recsys.get_recommendations_cf(customer_id, n_recommendations=5)\n",
    "    print(f\"Customer {customer_id}: {recs}\")\n",
    "\n",
    "# Content-Based Filtering\n",
    "print(\"\\nðŸ“Š Building Content-Based Model...\")\n",
    "content_result = recsys.content_based_filtering(products_df)\n",
    "print(\"âœ“ Content-based similarity matrix computed\")\n",
    "\n",
    "# Hybrid Recommendations\n",
    "print(\"\\nðŸ”€ Generating Hybrid Recommendations...\")\n",
    "hybrid_recs = recsys.hybrid_recommendations(\n",
    "    user_id=1,\n",
    "    product_id=10,\n",
    "    similarity_matrix=content_result['similarity_matrix'],\n",
    "    cf_weight=0.6,\n",
    "    cb_weight=0.4,\n",
    "    n_recommendations=5\n",
    ")\n",
    "print(f\"Hybrid recommendations for user 1: {hybrid_recs}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nðŸ“ˆ Evaluating Recommendations...\")\n",
    "eval_metrics = recsys.evaluate_recommendations(purchases_df)\n",
    "print(f\"Precision: {eval_metrics['precision']:.3f}\")\n",
    "print(f\"Recall: {eval_metrics['recall']:.3f}\")\n",
    "print(f\"Coverage: {eval_metrics['coverage']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be89b68",
   "metadata": {},
   "source": [
    "## 5. Product Recommendation System\n",
    "\n",
    "Implementing collaborative filtering, content-based, and hybrid recommendation approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explanations\n",
    "import shap\n",
    "\n",
    "print(\"ðŸ“Š SHAP Feature Importance Analysis (XGBoost)...\\n\")\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_test[:100])  # Use subset for speed\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test[:100], feature_names=feature_cols, show=False, plot_type=\"bar\")\n",
    "plt.title(\"SHAP Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ SHAP analysis completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e21c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC Curves\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for model_name, predictions in churn_models.predictions.items():\n",
    "    y_pred_proba = predictions['y_pred_proba']\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=fpr, y=tpr,\n",
    "        mode='lines',\n",
    "        name=f'{model_name} (AUC={roc_auc:.3f})'\n",
    "    ))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    mode='lines', name='Random Classifier',\n",
    "    line=dict(dash='dash')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    xlabel='False Positive Rate',\n",
    "    ylabel='True Positive Rate',\n",
    "    title='ROC-AUC Curves for Churn Prediction Models',\n",
    "    hovermode='closest'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Feature Importance\n",
    "print(\"\\nðŸ” Feature Importance (XGBoost):\")\n",
    "importance = churn_models.get_feature_importance('xgboost', top_k=15)\n",
    "importance_df = pd.DataFrame(importance).T\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda5dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.churn import ChurnPredictionModels\n",
    "\n",
    "# Initialize churn models\n",
    "churn_models = ChurnPredictionModels()\n",
    "\n",
    "print(\"ðŸŽ¯ Training Churn Prediction Models...\\n\")\n",
    "\n",
    "# Train XGBoost\n",
    "print(\"1ï¸âƒ£ XGBoost Model\")\n",
    "xgb_model, xgb_metrics = churn_models.xgboost_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Train LightGBM\n",
    "print(\"\\n2ï¸âƒ£ LightGBM Model\")\n",
    "lgb_model, lgb_metrics = churn_models.lightgbm_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"\\n3ï¸âƒ£ Random Forest Model\")\n",
    "rf_model, rf_metrics = churn_models.random_forest_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Train Logistic Regression (Baseline)\n",
    "print(\"\\n4ï¸âƒ£ Logistic Regression (Baseline)\")\n",
    "lr_model, lr_metrics = churn_models.logistic_regression_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Compare Models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "comparison_df = churn_models.compare_models()\n",
    "print(comparison_df.to_string())\n",
    "\n",
    "# Visualize Comparison\n",
    "fig = px.bar(\n",
    "    comparison_df,\n",
    "    x='model',\n",
    "    y=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'],\n",
    "    barmode='group',\n",
    "    title='Churn Prediction Model Comparison'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1cb929",
   "metadata": {},
   "source": [
    "## 4. Churn Prediction Models\n",
    "\n",
    "Building multiple models with SMOTE for class imbalance, evaluation metrics, and SHAP explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction for Visualization\n",
    "print(\"ðŸŽ¨ Applying PCA for visualization...\")\n",
    "pca_result = apply_pca(X_seg, n_components=2)\n",
    "X_pca = pca_result['X_train']\n",
    "print(f\"âœ“ PCA Explained Variance: {pca_result['explained_variance']:.1%}\")\n",
    "\n",
    "print(\"\\nðŸŽ¨ Applying t-SNE for visualization...\")\n",
    "X_tsne = apply_tsne(X_seg, n_components=2, perplexity=30)\n",
    "print(\"âœ“ t-SNE completed\")\n",
    "\n",
    "# PCA Visualization\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(\"K-Means\", \"DBSCAN\", \"Hierarchical\"),\n",
    "    specs=[[{'type': 'scatter'}, {'type': 'scatter'}, {'type': 'scatter'}]]\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_pca[:, 0], y=X_pca[:, 1], mode='markers', \n",
    "               marker=dict(color=kmeans_labels, colorscale='Viridis'),\n",
    "               name='K-Means'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_pca[:, 0], y=X_pca[:, 1], mode='markers',\n",
    "               marker=dict(color=dbscan_labels, colorscale='Plasma'),\n",
    "               name='DBSCAN'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_pca[:, 0], y=X_pca[:, 1], mode='markers',\n",
    "               marker=dict(color=hierarchical_labels, colorscale='Viridis'),\n",
    "               name='Hierarchical'),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"Clustering Results (PCA Projection)\")\n",
    "fig.show()\n",
    "\n",
    "# Cluster Profiles\n",
    "print(\"\\nðŸ“Š Cluster Profiles (K-Means):\")\n",
    "profiles = seg.get_cluster_profiles(customers_engineered, kmeans_labels)\n",
    "print(profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.segmentation import CustomerSegmentation\n",
    "from src.features.engineering import apply_pca, apply_tsne\n",
    "\n",
    "# Get segmentation features\n",
    "X_seg = fe.get_segmentation_features(customers_engineered, fit_scaler=True)\n",
    "\n",
    "# Initialize segmentation\n",
    "seg = CustomerSegmentation()\n",
    "\n",
    "# 1. K-Means Clustering\n",
    "print(\"ðŸŽ¯ Training K-Means Clustering...\")\n",
    "kmeans_result = seg.kmeans_clustering(X_seg, n_clusters=5, n_init=10)\n",
    "kmeans_labels = kmeans_result['labels']\n",
    "print(f\"âœ“ K-Means Metrics: {kmeans_result['metrics']}\")\n",
    "\n",
    "# 2. DBSCAN Clustering\n",
    "print(\"\\nðŸŽ¯ Training DBSCAN Clustering...\")\n",
    "dbscan_result = seg.dbscan_clustering(X_seg, eps=1.5, min_samples=5)\n",
    "dbscan_labels = dbscan_result['labels']\n",
    "print(f\"âœ“ DBSCAN Metrics: {dbscan_result['metrics']}\")\n",
    "\n",
    "# 3. Hierarchical Clustering\n",
    "print(\"\\nðŸŽ¯ Training Hierarchical Clustering...\")\n",
    "hierarchical_result = seg.hierarchical_clustering(X_seg, n_clusters=5, linkage='ward')\n",
    "hierarchical_labels = hierarchical_result['labels']\n",
    "print(f\"âœ“ Hierarchical Metrics: {hierarchical_result['metrics']}\")\n",
    "\n",
    "# Find optimal clusters\n",
    "print(\"\\nðŸ” Finding optimal number of clusters...\")\n",
    "optimal = seg.find_optimal_clusters(X_seg, max_k=10)\n",
    "optimal_k = optimal['optimal_k']\n",
    "print(f\"âœ“ Optimal K: {optimal_k}\")\n",
    "\n",
    "# Visualize optimal K\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=optimal['inertias'], mode='lines+markers', name='Inertia'))\n",
    "fig.add_trace(go.Scatter(y=optimal['silhouettes'], mode='lines+markers', name='Silhouette', yaxis='y2'))\n",
    "fig.update_layout(\n",
    "    yaxis=dict(title='Inertia'),\n",
    "    yaxis2=dict(title='Silhouette Score', overlaying='y', side='right'),\n",
    "    title='Elbow Method & Silhouette Analysis'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878eafb1",
   "metadata": {},
   "source": [
    "## 3. Customer Segmentation with Clustering\n",
    "\n",
    "Implementing multiple clustering algorithms with visualization and evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.engineering import FeatureEngineer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize feature engineer\n",
    "fe = FeatureEngineer()\n",
    "\n",
    "# Create engineered features\n",
    "print(\"ðŸ”§ Engineering features...\")\n",
    "customers_engineered = fe.create_customer_features(customers_df)\n",
    "\n",
    "print(\"âœ“ Engineered features created:\")\n",
    "print(customers_engineered.columns.tolist())\n",
    "\n",
    "# Select features for modeling\n",
    "X, feature_cols = fe.prepare_for_modeling(customers_engineered, fit_scaler=True)\n",
    "y = customers_engineered['churned'].values\n",
    "\n",
    "print(f\"\\nðŸ“Š Feature matrix shape: {X.shape}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"âœ“ Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"âœ“ Churn in train: {y_train.mean():.1%}\")\n",
    "print(f\"âœ“ Churn in test: {y_test.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496668ee",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering & Preprocessing\n",
    "\n",
    "Creating advanced features for machine learning models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284bd4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "print(\"ðŸ“ˆ Basic Statistics:\")\n",
    "print(customers_df.describe())\n",
    "\n",
    "print(\"\\nðŸŽ¯ Churn Distribution:\")\n",
    "print(customers_df['churned'].value_counts())\n",
    "print(f\"Churn Rate: {customers_df['churned'].mean():.1%}\")\n",
    "\n",
    "# Correlation Analysis\n",
    "fig = px.imshow(\n",
    "    customers_df[['total_purchases', 'avg_order_value', 'days_active', \n",
    "                   'login_frequency', 'support_tickets', 'churned']].corr(),\n",
    "    title=\"Feature Correlation Matrix\",\n",
    "    labels=dict(color=\"Correlation\")\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Distribution plots\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=3,\n",
    "    subplot_titles=(\"Purchase Distribution\", \"Order Value\", \"Days Active\", \n",
    "                   \"Login Frequency\", \"Support Tickets\", \"Churn Rate\")\n",
    ")\n",
    "\n",
    "features = ['total_purchases', 'avg_order_value', 'days_active', \n",
    "            'login_frequency', 'support_tickets', 'churned']\n",
    "\n",
    "for idx, feature in enumerate(features, 1):\n",
    "    row = (idx - 1) // 3 + 1\n",
    "    col = (idx - 1) % 3 + 1\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=customers_df[feature], name=feature, nbinsx=30),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=600, showlegend=False, title_text=\"Feature Distributions\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45343af5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate Sample Customer Data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerate_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     generate_customer_data, generate_product_data, \n\u001b[32m      4\u001b[39m     generate_purchase_history, generate_reviews\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Generate datasets\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ“Š Generating customer datasets...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Generate Sample Customer Data\n",
    "from src.data.generate_data import (\n",
    "    generate_customer_data, generate_product_data, \n",
    "    generate_purchase_history, generate_reviews\n",
    ")\n",
    "\n",
    "# Generate datasets\n",
    "print(\"ðŸ“Š Generating customer datasets...\")\n",
    "customers_df = generate_customer_data(n_customers=2000)\n",
    "products_df = generate_product_data(n_products=100)\n",
    "purchases_df = generate_purchase_history(n_customers=2000, n_products=100, n_transactions=10000)\n",
    "reviews_df = generate_reviews(n_reviews=1000)\n",
    "\n",
    "print(f\"âœ“ Customers: {customers_df.shape}\")\n",
    "print(f\"âœ“ Products: {products_df.shape}\")\n",
    "print(f\"âœ“ Purchases: {purchases_df.shape}\")\n",
    "print(f\"âœ“ Reviews: {reviews_df.shape}\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Customer Data Sample:\")\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e92b8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# ==================== SECTION 1: Data Loading & EDA ====================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configure visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26218670",
   "metadata": {},
   "source": [
    "# Full-Stack Customer ML Platform ðŸš€\n",
    "\n",
    "## Customer Behavior Analysis, Segmentation & Churn Prediction\n",
    "\n",
    "This comprehensive notebook builds a production-ready AI system featuring:\n",
    "- **Customer Segmentation** (K-Means, DBSCAN, Hierarchical Clustering)\n",
    "- **Churn Prediction** (XGBoost, LightGBM, Random Forest + SMOTE)\n",
    "- **Product Recommendations** (Collaborative Filtering, Content-Based, Hybrid)\n",
    "- **NLP Sentiment Analysis** (Text preprocessing, feature extraction, classification)\n",
    "- **Model Deployment** (FastAPI real-time + batch serving)\n",
    "- **MLOps Pipeline** (MLflow tracking, model registry, Kubernetes deployment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
